For the love of all that is good and holy, don't use this in any malicious way.
If you have any conerns about wikipedia going down or the internet falling apart or 
for whatever reason you want to have a local backup of wikipedia, just run this. 
From the directory where you place the .py, it will make a new directory called "wikis",
in that folder it will store the stripped down text of each article.
To save time/space it only saves the core article, not the references and other info.
Ideally this should be executed with 256 petabytes of ram and download speed in the YBPS
Alternatively just let it run for a week or two. If there is an interrupt, it will wait 60
seconds and try again indefinitely. So if wikipedia blocks the request, it tries again a 
minute later and if that request is also blocked then it tries again a minute later etc
It's worth pointing out that as the cache of visited urls increases, the runtime 
increases as well. So for example the first million are faster than the second million,
and so on.

Note: the java version does not work, I stopped in middle and switched to python because
it has faster http requests and file writing. I timed both for the first 500 rounds and
python was much quicker. If you try to run the java version, you first need to remove 
forbidden symbols from the file name, something like this
    title = title.replace("\"", "").replace("\\", "").replace("|", "").replace("/", "").replace("*", "").replace("?", "")

PS this was made with python 2.7 because I can't use 3+ for different projects I'm working on
Also I'm not too sure if this will work altogether if you're not on a windows machine

PPS When I made this it was within the restrictions of the wiki/wikipedia/wikipedia terms of use, 
read them and make sure that it's still permitted before using it
